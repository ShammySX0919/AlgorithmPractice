General ideas:
--collect statistics of frequency, use hash map or trie tree.
--for big files, and limited memory, split file according to mod operation.
--for top K, in each of collection, it should keep K samples(least or most), then in final merge, pick final top k

1. find top 1 visiting IP from huge volume of log
    --IP assuming is v4, then totall possible 2^32=4G number of addresses
    --for given date's log, split the log into 1000 smaller files. distrubute content by ip%1000, then each file contains 
    at most 4M ip addresses, which can be performed by most of computers.
    --use hashmap to collect statistics of counting of IP, remember the most frequently visiting one
    --gather top ip from 1000 processes and sort it normally to get the most frequencly visiting one
    
2. given 1G search record, find top K mostly searched terms. most of search are duplicate, so that about 300m unique terms.
    --map string/record to hash, reduce the memory usage
    --use hashmap to collect statistics of search frequency on terms. this will use O(N) time and O(N) space
    --use a min heap of size K (insert first K to it), come up a comparator on map.Entry that compares the counter of an entry.
    --for each entry in map, put it to min heap: insert first K, from K+1 on, compare it with peeking of minHeap, if it is bigger,
       then poll element from minHeap and insert current element to minHeap, this will be log(K) each element, so N*log(K) in totall
    --finally, retrieve elements from minHeap to get top K in ascending order
    2.2 use tri tree, keep counter for each of string, then use minHeap to figure out top K
3. A file of 1Gb, one word in a row, max length of word is 16bytes. memory is limited to 1Mb. return top 100 most frequent words.
    --1mb/16/8=7812.5 that means 1mb can handle any split with 7812 records. 
    --let's split file into multiple files with each file contains 7000 records or any number that is smaller depending memroy overhead you are facing.
    --for each file, figure out top 100 records, if less than 100 records, return all records. write them into a file.
    (hashmap, trie tree, and minHeap of 100)
    --for each of output file, do sort merge, until all files are merged.
    --get top 100 from merged file.
    
